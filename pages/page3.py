# Import des biblioth√®ques n√©cessaires
from langchain_community.vectorstores import Chroma  # Pour la base de donn√©es vectorielle
from langchain_openai import OpenAIEmbeddings     # Pour convertir le texte en vecteurs
from openai import OpenAI                        # Client OpenAI pour Grok
from langchain_openai import ChatOpenAI          # Pour utiliser Grok avec LangChain
from langchain.memory import ConversationBufferMemory  # Pour g√©rer l'historique des conversations
from langchain_core.prompts import PromptTemplate    # Pour structurer les prompts
from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain  # Pour combiner recherche et conversation
from dotenv import load_dotenv  # Pour g√©rer les variables d'environnement
import streamlit as st  # Pour l'interface utilisateur
import os
from langchain_groq import ChatGroq

# Chargement des variables d'environnement (cl√©s API, etc.)
load_dotenv()

# Initialisation du mod√®le d'embedding
@st.cache_resource  # Cache la ressource pour √©viter de la recharger √† chaque requ√™te
def get_embeddings():
    """
    Initialise le mod√®le d'embedding d'OpenAI.
    
    Les embeddings sont des repr√©sentations vectorielles du texte qui permettent
    de mesurer la similarit√© s√©mantique entre diff√©rents textes.
    
    Returns:
        OpenAIEmbeddings: Instance du mod√®le d'embedding configur√©
    """
    if not os.getenv("OPENAI_API_KEY"):
        raise ValueError("OPENAI_API_KEY non trouv√©e dans les variables d'environnement")
    return OpenAIEmbeddings(
        model="text-embedding-3-small"  # Mod√®le plus l√©ger et √©conomique
    )

# Initialisation du mod√®le de langage (LLM)
@st.cache_resource
def get_llm():
    """
    Initialise le mod√®le de langage Grok.
    
    Grok est un mod√®le de langage d√©velopp√© par xAI,
    offrant des performances de haut niveau pour la g√©n√©ration de texte.
    
    Returns:
        ChatOpenAI: Instance du mod√®le de langage configur√©
    """
    if not os.getenv("GROQ_API_KEY"):
        raise ValueError("GROQ_API_KEYY non trouv√©e dans les variables d'environnement")
    
    return ChatGroq(
        temperature=0.7,  # Contr√¥le la cr√©ativit√© des r√©ponses (0=conservateur, 1=cr√©atif)
        model_name="mixtral-8x7b-32768",  # Mod√®le Grok
        api_key=os.getenv("GROQ_API_KEY"),
    )

# Chargement de la base de donn√©es vectorielle
@st.cache_resource
def get_vectorstore():
    """
    Charge la base de donn√©es vectorielle Chroma.
    
    Chroma est une base de donn√©es vectorielle qui stocke les embeddings des documents
    et permet de faire des recherches par similarit√© s√©mantique.
    
    Returns:
        Chroma: Instance de la base de donn√©es vectorielle
    """
    embeddings = get_embeddings()
    vectorstore = Chroma(
        persist_directory="chroma_db",  # Dossier o√π sont stock√©s les vecteurs
        embedding_function=embeddings
    )
    return vectorstore

# Cr√©ation de la cha√Æne de conversation
def get_conversation_chain(vectorstore):
    """
    Configure la cha√Æne de conversation qui combine recherche et dialogue.
    
    Cette fonction:
    1. Initialise la m√©moire pour garder le contexte de la conversation
    2. Cr√©e un template de prompt qui guide le comportement de l'assistant
    3. Configure la cha√Æne de conversation qui utilise le LLM et la recherche
    
    Args:
        vectorstore: Base de donn√©es vectorielle pour la recherche
        
    Returns:
        ConversationalRetrievalChain: Cha√Æne de conversation configur√©e
    """
    # Initialisation de la m√©moire pour le contexte
    memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True
    )

# Cr√©ation du template de prompt
    template = """Tu es un assistant de l'industrie cin√©matographique sympathique et comp√©tent. Utilise les √©l√©ments de contexte suivants pour 
    fournir des recommandations de films et des conseils de information utiles. 
    
    Pour chaque film, utilise ce format markdown:
    ### üìº [Nom de la film]
    **üï¶ Run time minutes:** [runtimeMinutes]
    **üìä Average rating:** [averageRating]
    **üé¨ Types:**[genres]
    ** üí≤ Budget:**[budget]
    **‚≠ê Popularity:**[popularity_film]
    
    #### üèïÔ∏è Poster
    -[poster_path]
    
    #### üìù Description
    [Overview]
    
    ---
    
    Instructions importantes:
    - Ne jamais inclure d'URLs ou de liens vers des sites externes dans tes r√©ponses
    - R√©ponds toujours en fran√ßais
    - Utilise du markdown avec des titres (##, ###), des listes (- ou *), et du texte en gras (**) ou en italique (*) quand c'est appropri√©

    <contexte> 
    {context}
    </contexte>
    
    Historique de conversation: {chat_history}
    
    Humain: {question}
    
    Assistant: Je vais t'aider avec √ßa."""


    prompt = PromptTemplate(
            template=template,
            input_variables=["context", "chat_history", "question"]
        )
        
    # Configuration de la cha√Æne de conversation
    conversation_chain = ConversationalRetrievalChain.from_llm(
        llm=get_llm(),
        retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),  # R√©cup√®re les 3 documents les plus pertinents
        memory=memory,
        combine_docs_chain_kwargs={"prompt": prompt}
    )
    
    return conversation_chain

def main():
    """
    Fonction principale qui configure et lance l'interface utilisateur Streamlit.
    
    Cette fonction:
    1. Configure la page Streamlit
    2. Initialise les composants n√©cessaires (vectorstore, conversation)
    3. G√®re l'interface utilisateur et les interactions
    """
    # Configuration de la page Streamlit
    st.set_page_config(
        page_title="Assistant cin√©matographique",
        page_icon="üé¨",
        layout="wide"
    )
    
    st.title("üé• Assistant cin√©matographique LangChain")
    
    # Initialisation des variables de session
    if "conversation" not in st.session_state:
        st.session_state.conversation = None
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    # Initialisation des composants
    try:
        # Chargement de la base de donn√©es vectorielle
        vectorstore = get_vectorstore()
        if st.session_state.conversation is None:
            st.session_state.conversation = get_conversation_chain(vectorstore)
        
        # Interface de chat
        st.markdown("""
        ### üëã Bienvenue sur votre Assistant cin√©matographique IA !
        Je peux vous aider √† trouver des films, donner des conseils de cin√©matographique et r√©pondre √† vos questions film.
        Essayez de me demander par exemple :
        - "Je veux regarder le film d'action"
        - "Quelle est la dur√©e du film ?"
        - "Donne-moi un film avec un rating superieur √† 8"
        """)

        # Champ de saisie utilisateur
        user_input = st.text_input(
            "Que souhaitez-vous regarder aujourd'hui ?",
            key="user_input", 
            placeholder="Posez-moi n'importe quelle question sur la cin√©matographique ou les films !"
        )

        if user_input:
            # Traitement de la requ√™te utilisateur
            with st.spinner('Recherche de le meilleur film...'):
                response = st.session_state.conversation.invoke({
                    "question": user_input,
                    "chat_history": st.session_state.chat_history
                })
                
                # Mise √† jour de l'historique
                st.session_state.chat_history.append((user_input, response["answer"]))

            # Affichage de la r√©ponse avec formatage markdown
            st.markdown('<div class="chat-message assistant-message">', unsafe_allow_html=True)
            st.markdown(response["answer"])
            st.markdown('</div>', unsafe_allow_html=True)

            # Affichage de l'historique des conversations dans un expander
            with st.expander("üí¨ Historique des conversations", expanded=False):
                for i, (user_msg, ai_msg) in enumerate(reversed(st.session_state.chat_history)):
                    # Message utilisateur
                    st.markdown(f'<div class="chat-message-user">', unsafe_allow_html=True)
                    st.markdown(f"**üë§ Vous:** {user_msg}", unsafe_allow_html=True)
                    st.markdown('</div>', unsafe_allow_html=True)
                    
                    # Message assistant
                    st.markdown(f'<div class="chat-message-assistant">', unsafe_allow_html=True)
                    st.markdown(f"**ü§ñ Assistant:** {ai_msg}", unsafe_allow_html=True)
                    st.markdown('</div>', unsafe_allow_html=True)
                    
                    # S√©parateur entre les conversations
                    if i < len(st.session_state.chat_history) - 1:
                        st.markdown("<hr style='margin: 15px 0; border: none; border-top: 1px solid #eee;'>", unsafe_allow_html=True)

    except Exception as e:
        # Gestion des erreurs
        st.error("‚ö†Ô∏è Erreur de Configuration")
        st.error(f"Une erreur s'est produite : {str(e)}")
        st.markdown("""
        Veuillez v√©rifier :
        1. Que toutes les cl√©s API requises sont d√©finies dans le fichier `.env`
        2. Que le vectorstore a √©t√© g√©n√©r√© en utilisant `generate_vectorstore.py`
        3. Que les d√©pendances n√©cessaires sont install√©es
        """)

if __name__ == "__main__":
    main()

col3, col4, col5=st.columns(3)
with col4:
    if st.button(":orange[Voulez-vous s√©lectionner un autre film?]", key="reset_button"):
        st.session_state['conversation'] = "reset"  # R√©initialise l'√©tat
        st.switch_page("page_acceuil.py")  # Redirige vers la page d'accueil

        